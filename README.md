# Machine-Learning-Models
Implementation of traditional ML models


Classifiers
These are models designed to predict discrete outcomes or categories. Examples include spam detection, disease diagnosis, or image classification.

Popular Classification Models
Logistic Regression:

Simple and interpretable linear model for binary or multinomial classification.
Output: Probabilities for each class.
K-Nearest Neighbors (KNN):

Instance-based learning; predicts the class based on the majority class of its nearest neighbors.
Suitable for smaller datasets.
Support Vector Machine (SVM):

Finds the hyperplane that best separates classes.
Kernel trick enables handling non-linear separations.
Decision Trees:

Rule-based classifier; splits data based on feature thresholds.
Easy to interpret but prone to overfitting.
Random Forest:

Ensemble of decision trees; reduces overfitting and improves generalization.
Works well with a mix of categorical and continuous data.
Gradient Boosting Machines (e.g., XGBoost, LightGBM):

Builds models sequentially to correct errors of prior models.
High accuracy but computationally intensive.
Naive Bayes:

Probabilistic classifier based on Bayes' theorem; assumes feature independence.
Effective for text classification tasks.
Neural Networks:

Includes feedforward and convolutional neural networks (CNNs).
Suitable for high-dimensional data like images, audio, and video.
k-Means (Semi-Supervised):

Often used as a clustering tool but can also assist classification tasks with semi-supervised learning.
Classification Metrics